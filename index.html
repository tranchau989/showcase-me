<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>GitHub Project Showcase</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f4f4f4;
      margin: 0;
      padding: 20px;
    }

    .container {
      max-width: 800px;
      margin: auto;
    }

    h1 {
      text-align: center;
      color: #333;
    }

    .project {
      background: #fff;
      border-radius: 8px;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
      padding: 20px;
      margin-bottom: 20px;
    }

    .project h2 {
      margin-top: 0;
      color: #0366d6;
    }

    .project p {
      color: #555;
    }

    .project a {
      color: #0366d6;
      text-decoration: none;
    }

    .project a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>My GitHub Projects</h1>

    <div class="project">
      <h2><a href="https://github.com/tranchau989/Snapshot_AcousticImpedance_Prediction" target="_blank">Project One: Acoustic Impedant Prediction using Stratigraphy-guided Deep Learning</a></h2>
        <p>This research presents a new method—<strong>Stratigraphy-Guided Deep Learning (SGDL)</strong>—for estimating acoustic impedance from poststack seismic data using deep neural networks. Acoustic impedance plays a key role in linking seismic reflectivity to subsurface rock properties, and accurate inversion is critical for reservoir characterization.
        </p>
        <p>SGDL advances seismic inversion by incorporating stratigraphic units as an additional input feature, allowing the model to learn in a more geologically meaningful context. The approach leverages a <strong>Temporal Convolutional Network (TCN)</strong> as the backbone architecture, which is specifically suited to capturing both temporal and spatial features in seismic data. TCNs process sequential information along the depth axis (time/depth domain), extracting subsurface trends and reflectivity patterns, while also integrating lateral information from neighboring traces. This dual capability enables the model to better understand local continuity and geological structures—key factors in inversion quality.
        </p>
        <p>In a case study using the Volve dataset, SGDL was trained on well-controlled field data and benchmarked against traditional poststack inversion and other deep learning methods. The results demonstrate:
        </p>
        <ul>
          <li>Approximately 10% improvement in Pearson Correlation on two benchmark wells.</li>
          <li>Approximately 20% average improvement across 10 test wells, showing strong generalization and robustness.</li>
        </ul>
        <p><strong>Why this matters:</strong><br>
          SGDL is especially effective in near-field exploration settings, where labeled well data and stratigraphic interpretations are available. By combining geological priors with a model capable of extracting rich temporal and spatial features, SGDL offers a robust and interpretable solution for seismic inversion.
        </p>
    </div>

    <div class="project">
      <h2><a href="https://github.com/AchillesProject/EvolvedNeuron_for_HAR" target="_blank">Project Two: Introducing an Evolved Neuron for Human Activity Recognition</a></h2>
      <p>As an outcome of the Information Technology and Communication Department’s internal project in Østfold University College, multiple evolved recursive neuron networks were generated for Human Activity Recognition (HAR) and other applications. They were synthesized by the Automatic Design of Algorithms Through Evolution (ADATE) system based on the Recurrent Neural Network (RNN), in particular the Long Short-Term Memory (LSTM) model’s logic. Their transcripts are written in the Standard ML language, which is not well-known or commonly used in applied machine learning. Moreover, the result’s reliability remains open. Correspondingly, the project is established as a pilot study that concentrates on neuron exploration, which is comprised of analyzing its logic, importing it to other frameworks such as TensorFlow, and evaluating its accuracy. The selected neuron is version 30th in the synthesized list for HAR using the Wireless Sensor Data Mining (WISDM) dataset, which is the second best in both complexity and accuracy.</p>
    </div>

    <div class="project">
      <h2><a href="https://github.com/tranchau989/showcase-me/tree/main/prj_snapshots" target="_blank">Snapshots of other works</a></h2>
      <p>This item contains a collection of standalone exploratory projects in the field of computer vision. Each Jupyter notebook addresses a unique research question or application, ranging from image fusion techniques to biometric recognition tasks. These projects are independent studies and are not part of a single workflow. Together, they reflect a diverse exploration of classical image processing methods as well as modern machine learning techniques. This folder may be of interest to those working on multimodal image analysis, hand-based biometrics, or visual data preprocessing for classification models.
      </p>
      <p>The <a href="https://github.com/tranchau989/showcase-me/blob/main/prj_snapshots/00_Images_Fusing.ipynb" target="_blank"><strong>00_Images_Fusing.ipynb</strong></a> notebook presents an advanced study on multiview image fusion using modern computer vision techniques, including the integration of Segment Anything Model v2 (SAM2) and RoMA for image matching and alignment. The objective of this notebook is to automatically segment relevant visual content from different image views, match features across them, and perform perspective correction to generate clean, rectified views suitable for fusion. It includes GPU-accelerated inference with support for both CUDA and Apple MPS, and leverages deep segmentation and matching models for region extraction and image warping. This notebook is aimed at researchers or developers interested in state-of-the-art image fusion for surveillance, robotics, or scene understanding tasks where precise matching and alignment from multiple views is critical.
      </p>
      <p>The <a href="https://github.com/tranchau989/showcase-me/blob/main/prj_snapshots/01_Nora_Fusion_clean.ipynb" target="_blank"><strong>01_Nora_Fusion_clean.ipynb</strong></a> notebook presents a multimodal learning experiment using the MNIST dataset as a testbed. Each MNIST image is split into upper and lower halves to simulate two distinct modalities. The study trains three different models: a CNN encoder for the upper half, an MLP encoder for the lower half, and a fused model that combines both representations. The objective is to compare unimodal versus multimodal performance in digit classification tasks. The notebook also explores different fusion strategies—such as feature concatenation and averaging—and evaluates the robustness of each model by injecting noise into one or both modalities. Techniques like PCA and t-SNE are employed to visualize representation spaces. This project is valuable for anyone studying representation learning, model fusion, and noise resilience in multi-view learning contexts.
      </p>
      <p>The notebook, <a href="https://github.com/tranchau989/showcase-me/blob/main/prj_snapshots/02_handrecognition.ipynb" target="_blank"><strong>02_handrecognition.ipynb</strong></a>, explores the task of multimodal hand gesture recognition with a primary focus on evaluating different encoder architectures and fusion strategies for combining image and audio data. It begins by preprocessing grayscale hand images and audio waveforms, converting the latter into mel spectrograms to extract meaningful auditory features. After confirming that the labels for both modalities are aligned, the notebook defines a custom dataset structure to jointly access image, audio, and label data during training. The central objective is to systematically test how different modality-specific encoders—such as convolutional neural networks (CNNs) and multilayer perceptrons (MLPs)—perform when paired with various fusion techniques, including early fusion, late fusion, and more sophisticated learned fusion mechanisms. By isolating the effects of each encoder and fusion strategy, the work aims to understand how different design choices influence the ability to effectively integrate features from images with patterns from audio.
      </p>
    </div>
  </div>
</body>
</html>

