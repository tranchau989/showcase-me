<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>GitHub Project Showcase</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f4f4f4;
      margin: 0;
      padding: 20px;
    }

    .container {
      max-width: 800px;
      margin: auto;
    }

    h1 {
      text-align: center;
      color: #333;
    }

    .project {
      background: #fff;
      border-radius: 8px;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
      padding: 20px;
      margin-bottom: 20px;
    }

    .project h2 {
      margin-top: 0;
      color: #0366d6;
    }

    .project p {
      color: #555;
    }

    .project a {
      color: #0366d6;
      text-decoration: none;
    }

    .project a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>My GitHub Projects</h1>

    <div class="project">
      <h2><a href="https://github.com/tranchau989/Snapshot_AcousticImpedance_Prediction" target="_blank">Project One: Acoustic Impedant Prediction using Stratigraphy-guided Deep Learning</a></h2>
      This research presents a new method—Stratigraphy-Guided Deep Learning (SGDL)—for estimating acoustic impedance from poststack seismic data using deep neural networks. Acoustic impedance plays a key role in linking seismic reflectivity to subsurface rock properties, and accurate inversion is critical for reservoir characterization.

SGDL advances seismic inversion by incorporating stratigraphic units as an additional input feature, allowing the model to learn in a more geologically meaningful context. The approach leverages a Temporal Convolutional Network (TCN) as the backbone architecture, which is specifically suited to capturing both temporal and spatial features in seismic data. TCNs process sequential information along the depth axis (time/depth domain), extracting subsurface trends and reflectivity patterns, while also integrating lateral information from neighboring traces. This dual capability enables the model to better understand local continuity and geological structures—key factors in inversion quality.
In a case study using the Volve dataset, SGDL was trained on well-controlled field data and benchmarked against traditional poststack inversion and other deep learning methods. The results demonstrate:
  <ul>
    <li>A ~10% improvement in Pearson Correlation on two benchmark wells.</li>
    <li>A ~20% average improvement across 10 test wells, showing strong generalization and robustness.</li>
  </ul>
Why this matters:
SGDL is especially effective in near-field exploration settings, where labeled well data and stratigraphic interpretations are available. By combining geological priors with a model capable of extracting rich temporal and spatial features, SGDL offers a robust and interpretable solution for seismic inversion.
    </div>

    <div class="project">
      <h2><a href="https://github.com/AchillesProject/EvolvedNeuron_for_HAR" target="_blank">Project Two: Introducing an Evolved Neuron for Human Activity Recognition</a></h2>
      <p>As an outcome of the Information Technology and Communication Department’s internal project in Østfold University College, multiple evolved recursive neuron networks were generated for Human Activity Recognition (HAR) and other applications. They were synthesized by the Automatic Design of Algorithms Through Evolution (ADATE) system based on the Recurrent Neural Network (RNN), in particular the Long Short-Term Memory (LSTM) model’s logic. Their transcripts are written in the Standard ML language, which is not well-known or commonly used in applied machine learning. Moreover, the result’s reliability remains open. Correspondingly, the project is established as a pilot study that concentrates on neuron exploration, which is comprised of analyzing its logic, importing it to other frameworks such as TensorFlow, and evaluating its accuracy. The selected neuron is version 30th in the synthesized list for HAR using the Wireless Sensor Data Mining (WISDM) dataset, which is the second best in both complexity and accuracy.</p>
    </div>

    <div class="project">
      <h2><a href="https://github.com/tranchau989/showcase-me/tree/main/prj_snapshots" target="_blank">Project Snapshots of other works</a></h2>
      <p>This item contains a collection of standalone exploratory projects in the field of computer vision. Each Jupyter notebook addresses a unique research question or application, ranging from image fusion techniques to biometric recognition tasks. These projects are independent studies and are not part of a single workflow. Together, they reflect a diverse exploration of classical image processing methods as well as modern machine learning techniques. This folder may be of interest to those working on multimodal image analysis, hand-based biometrics, or visual data preprocessing for classification models.
      </p>
      <p>The <strong>00_Images_Fusing.ipynb</strong> focuses on the challenge of combining multiple images into a single, enhanced representation using various fusion strategies. The study begins with standard preprocessing techniques such as grayscale conversion and intensity normalization before exploring methods like pixel averaging, principal component analysis (PCA), wavelet transform fusion, and saliency-based fusion. The objective is to compare how well each technique preserves key features of the original inputs, both visually and using quantitative measures like entropy and edge preservation. This notebook is well-suited for anyone investigating image enhancement through data fusion in applications such as surveillance or remote sensing.
      </p>
      <p>The <strong>01_Nora_Fusion_clean.ipynb</strong> notebook presents a focused study on the fusion of thermal and RGB hand images, likely from the Nora dataset. The primary goal is to generate fused images that retain both thermal contours and visible details to support downstream biometric applications. This study includes image alignment, histogram equalization, and noise filtering to prepare data for fusion. It then applies carefully selected techniques to combine modalities and produces fused outputs suitable for tasks such as identity recognition or gesture detection. This project offers insights into the integration of multi-sensor data in biometric systems.
      </p>
      <p>The notebook, <strong>02_handrecognition.ipynb</strong>, is an independent study focused on recognizing hands using machine learning techniques. It explores the use of both traditional and deep learning-based feature extraction, including Histogram of Oriented Gradients (HOG) and convolutional neural networks like VGG16 and ResNet. These features are used to train classifiers such as support vector machines (SVM), random forests, and fine-tuned CNN models. The study assesses classification performance using accuracy metrics and confusion matrices, offering a comparative analysis of approaches for hand recognition tasks. This notebook is valuable for those interested in biometric classification using visual data.
      </p>
    </div>
  </div>
</body>
</html>

